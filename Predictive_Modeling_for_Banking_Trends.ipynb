{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/uJzDdBvhtrZGnhxQLE4f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayush245101/Distributed-Machine-Learning/blob/main/Predictive_Modeling_for_Banking_Trends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning with Apache Spark MLlib (Bank Dataset)\n",
        "\n",
        "Goal: Predict if a client will subscribe to a term deposit (y = yes/no)."
      ],
      "metadata": {
        "id": "gmZHp2yKR9dG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Setup PySpark Environment"
      ],
      "metadata": {
        "id": "lhqDOuIsSgC0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx2vA0M4RzKT",
        "outputId": "03e22657-62bf-449b-90ad-6edf8db3cf93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,434 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,123 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,825 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,526 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,856 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,969 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,168 kB]\n",
            "Fetched 37.2 MB in 4s (10.3 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  ca-certificates-java fonts-dejavu-core fonts-dejavu-extra java-common\n",
            "  libatk-wrapper-java libatk-wrapper-java-jni libpcsclite1 libxt-dev libxtst6\n",
            "  libxxf86dga1 openjdk-11-jdk-headless openjdk-11-jre openjdk-11-jre-headless\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  default-jre pcscd libxt-doc openjdk-11-demo openjdk-11-source visualvm\n",
            "  libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  ca-certificates-java fonts-dejavu-core fonts-dejavu-extra java-common\n",
            "  libatk-wrapper-java libatk-wrapper-java-jni libpcsclite1 libxt-dev libxtst6\n",
            "  libxxf86dga1 openjdk-11-jdk openjdk-11-jdk-headless openjdk-11-jre\n",
            "  openjdk-11-jre-headless x11-utils\n",
            "0 upgraded, 15 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 122 MB of archives.\n",
            "After this operation, 274 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 java-common all 0.72build2 [6,782 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpcsclite1 amd64 1.9.5-3ubuntu1 [19.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [42.6 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ca-certificates-java all 20190909ubuntu1.2 [12.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1 [214 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [73.6 MB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1 [1,342 kB]\n",
            "Fetched 122 MB in 4s (28.4 MB/s)\n",
            "Selecting previously unselected package java-common.\n",
            "(Reading database ... 125082 files and directories currently installed.)\n",
            "Preparing to unpack .../00-java-common_0.72build2_all.deb ...\n",
            "Unpacking java-common (0.72build2) ...\n",
            "Selecting previously unselected package libpcsclite1:amd64.\n",
            "Preparing to unpack .../01-libpcsclite1_1.9.5-3ubuntu1_amd64.deb ...\n",
            "Unpacking libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "Preparing to unpack .../02-openjdk-11-jre-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package ca-certificates-java.\n",
            "Preparing to unpack .../03-ca-certificates-java_20190909ubuntu1.2_all.deb ...\n",
            "Unpacking ca-certificates-java (20190909ubuntu1.2) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../04-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../05-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../06-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../07-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../08-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../09-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../10-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../11-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../12-openjdk-11-jre_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
            "Preparing to unpack .../13-openjdk-11-jdk-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../14-openjdk-11-jdk_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up java-common (0.72build2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up ca-certificates-java (20190909ubuntu1.2) ...\n",
            "head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
            "Adding debian:SecureSign_RootCA11.pem\n",
            "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
            "Adding debian:AffirmTrust_Commercial.pem\n",
            "Adding debian:DigiCert_Global_Root_G3.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
            "Adding debian:certSIGN_Root_CA_G2.pem\n",
            "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
            "Adding debian:QuoVadis_Root_CA_3.pem\n",
            "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
            "Adding debian:D-TRUST_EV_Root_CA_1_2020.pem\n",
            "Adding debian:HARICA_TLS_RSA_Root_CA_2021.pem\n",
            "Adding debian:Trustwave_Global_Certification_Authority.pem\n",
            "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
            "Adding debian:Comodo_AAA_Services_root.pem\n",
            "Adding debian:Certum_EC-384_CA.pem\n",
            "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
            "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
            "Adding debian:GlobalSign_Root_CA.pem\n",
            "Adding debian:HiPKI_Root_CA_-_G1.pem\n",
            "Adding debian:emSign_Root_CA_-_G1.pem\n",
            "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
            "Adding debian:CFCA_EV_ROOT.pem\n",
            "Adding debian:COMODO_Certification_Authority.pem\n",
            "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
            "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
            "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
            "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
            "Adding debian:Izenpe.com.pem\n",
            "Adding debian:HARICA_TLS_ECC_Root_CA_2021.pem\n",
            "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
            "Adding debian:Security_Communication_RootCA2.pem\n",
            "Adding debian:UCA_Global_G2_Root.pem\n",
            "Adding debian:D-TRUST_BR_Root_CA_1_2020.pem\n",
            "Adding debian:Secure_Global_CA.pem\n",
            "Adding debian:GTS_Root_R3.pem\n",
            "Adding debian:ISRG_Root_X1.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority.pem\n",
            "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
            "Adding debian:GlobalSign_Root_E46.pem\n",
            "Adding debian:vTrus_Root_CA.pem\n",
            "Adding debian:TWCA_Root_Certification_Authority.pem\n",
            "Adding debian:AffirmTrust_Premium.pem\n",
            "Adding debian:XRamp_Global_CA_Root.pem\n",
            "Adding debian:Starfield_Class_2_CA.pem\n",
            "Adding debian:Buypass_Class_2_Root_CA.pem\n",
            "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
            "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
            "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:Amazon_Root_CA_2.pem\n",
            "Adding debian:GLOBALTRUST_2020.pem\n",
            "Adding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\n",
            "Adding debian:certSIGN_ROOT_CA.pem\n",
            "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
            "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
            "Adding debian:ePKI_Root_Certification_Authority.pem\n",
            "Adding debian:Certum_Trusted_Root_CA.pem\n",
            "Adding debian:Security_Communication_ECC_RootCA1.pem\n",
            "Adding debian:Amazon_Root_CA_1.pem\n",
            "Adding debian:ACCVRAIZ1.pem\n",
            "Adding debian:QuoVadis_Root_CA_2.pem\n",
            "Adding debian:TWCA_Global_Root_CA.pem\n",
            "Adding debian:Amazon_Root_CA_3.pem\n",
            "Adding debian:emSign_Root_CA_-_C1.pem\n",
            "Adding debian:DigiCert_Global_Root_CA.pem\n",
            "Adding debian:Security_Communication_RootCA3.pem\n",
            "Adding debian:UCA_Extended_Validation_Root.pem\n",
            "Adding debian:GTS_Root_R1.pem\n",
            "Adding debian:Baltimore_CyberTrust_Root.pem\n",
            "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
            "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
            "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
            "Adding debian:NAVER_Global_Root_Certification_Authority.pem\n",
            "Adding debian:GTS_Root_R4.pem\n",
            "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:Buypass_Class_3_Root_CA.pem\n",
            "Adding debian:e-Szigno_Root_CA_2017.pem\n",
            "Adding debian:Telia_Root_CA_v2.pem\n",
            "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
            "Adding debian:Certainly_Root_E1.pem\n",
            "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
            "Adding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\n",
            "Adding debian:AffirmTrust_Networking.pem\n",
            "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
            "Adding debian:GlobalSign_Root_R46.pem\n",
            "Adding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\n",
            "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
            "Adding debian:Go_Daddy_Class_2_CA.pem\n",
            "Adding debian:Certigna_Root_CA.pem\n",
            "Adding debian:vTrus_ECC_Root_CA.pem\n",
            "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
            "Adding debian:NetLock_Arany_=Class_Gold=_FÅ‘tanÃºsÃ­tvÃ¡ny.pem\n",
            "Adding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\n",
            "Adding debian:SZAFIR_ROOT_CA2.pem\n",
            "Adding debian:Certum_Trusted_Network_CA.pem\n",
            "Adding debian:CA_Disig_Root_R2.pem\n",
            "Adding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\n",
            "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
            "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
            "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
            "Adding debian:GTS_Root_R2.pem\n",
            "Adding debian:ISRG_Root_X2.pem\n",
            "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
            "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
            "Adding debian:Actalis_Authentication_Root_CA.pem\n",
            "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
            "Adding debian:ANF_Secure_Server_Root_CA.pem\n",
            "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
            "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
            "Adding debian:DigiCert_Global_Root_G2.pem\n",
            "Adding debian:Security_Communication_Root_CA.pem\n",
            "Adding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\n",
            "Adding debian:DigiCert_TLS_RSA4096_Root_G5.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
            "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
            "Adding debian:SecureTrust_CA.pem\n",
            "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
            "Adding debian:Certainly_Root_R1.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
            "Adding debian:TunTrust_Root_CA.pem\n",
            "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
            "Adding debian:Certigna.pem\n",
            "Adding debian:Amazon_Root_CA_4.pem\n",
            "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
            "Adding debian:AffirmTrust_Premium_ECC.pem\n",
            "Adding debian:Atos_TrustedRoot_2011.pem\n",
            "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
            "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
            "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
            "Adding debian:Sectigo_Public_Server_Authentication_Root_R46.pem\n",
            "Adding debian:Atos_TrustedRoot_Root_CA_ECC_TLS_2021.pem\n",
            "Adding debian:Atos_TrustedRoot_Root_CA_RSA_TLS_2021.pem\n",
            "Adding debian:BJCA_Global_Root_CA2.pem\n",
            "Adding debian:BJCA_Global_Root_CA1.pem\n",
            "Adding debian:CommScope_Public_Trust_ECC_Root-01.pem\n",
            "Adding debian:Sectigo_Public_Server_Authentication_Root_E46.pem\n",
            "Adding debian:SSL.com_TLS_ECC_Root_CA_2022.pem\n",
            "Adding debian:SSL.com_TLS_RSA_Root_CA_2022.pem\n",
            "Adding debian:TrustAsia_Global_Root_CA_G4.pem\n",
            "Adding debian:CommScope_Public_Trust_RSA_Root-01.pem\n",
            "Adding debian:CommScope_Public_Trust_RSA_Root-02.pem\n",
            "Adding debian:TrustAsia_Global_Root_CA_G3.pem\n",
            "Adding debian:CommScope_Public_Trust_ECC_Root-02.pem\n",
            "done.\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for ca-certificates (20240203~22.04.1) ...\n",
            "Updating certificates in /etc/ssl/certs...\n",
            "0 added, 0 removed; done.\n",
            "Running hooks in /etc/ca-certificates/update.d...\n",
            "\n",
            "done.\n",
            "done.\n",
            "--2025-11-10 05:43:10--  https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400446614 (382M) [application/x-gzip]\n",
            "Saving to: â€˜spark-3.5.1-bin-hadoop3.tgzâ€™\n",
            "\n",
            "spark-3.5.1-bin-had 100%[===================>] 381.90M  4.98MB/s    in 42s     \n",
            "\n",
            "2025-11-10 05:43:52 (9.07 MB/s) - â€˜spark-3.5.1-bin-hadoop3.tgzâ€™ saved [400446614/400446614]\n",
            "\n",
            "âœ… Spark Session Created Successfully\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Setup PySpark in Google Colab\n",
        "\n",
        "# Install Java and Spark\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "\n",
        "# Remove any existing Spark installation and its archive to ensure a fresh setup\n",
        "!rm -rf /opt/spark\n",
        "!rm -f spark-3.5.1-bin-hadoop3.tgz\n",
        "\n",
        "# Download Spark binaries and extract them\n",
        "# Using a specific version (Spark 3.5.1 with Hadoop 3) that is known to work well in Colab environments\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!mv spark-3.5.1-bin-hadoop3 /opt/spark\n",
        "\n",
        "# Set environment variables for JAVA_HOME and SPARK_HOME\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
        "\n",
        "# Install findspark and pyspark\n",
        "!pip install -q findspark pyspark\n",
        "\n",
        "# Initialize findspark to enable PySpark to work with regular Python\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Initialize Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"BankMarketingAnalysis\").getOrCreate()\n",
        "\n",
        "print(\"âœ… Spark Session Created Successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load Dataset and Basic EDA"
      ],
      "metadata": {
        "id": "tPQ9gdCNSh8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/bank.csv\"  # Update path if needed\n",
        "\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "print(\"âœ… Data Loaded Successfully\")\n",
        "\n",
        "# Show first few rows\n",
        "df.show(5)\n",
        "df.printSchema()\n",
        "\n",
        "# Check dataset shape\n",
        "print(f\"Total Records: {df.count()}, Total Columns: {len(df.columns)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0lG4FmwSOQF",
        "outputId": "b0fde29b-63aa-4189-8907-0f494f4d31ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data Loaded Successfully\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "|age|        job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n",
            "| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n",
            "| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n",
            "| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|\n",
            "| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- y: string (nullable = true)\n",
            "\n",
            "Total Records: 4521, Total Columns: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Basic Data Exploration"
      ],
      "metadata": {
        "id": "GO7FkwVzSpp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of numeric features\n",
        "df.describe(['age', 'balance', 'duration', 'campaign']).show()\n",
        "\n",
        "# Target class distribution\n",
        "df.groupBy(\"y\").count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9m6_vtzSsgA",
        "outputId": "f56bbaa1-5d66-4a0a-db13-34ae34e2bdb9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+\n",
            "|summary|               age|           balance|          duration|          campaign|\n",
            "+-------+------------------+------------------+------------------+------------------+\n",
            "|  count|              4521|              4521|              4521|              4521|\n",
            "|   mean| 41.17009511170095|1422.6578190665782|263.96129174961294| 2.793629727936297|\n",
            "| stddev|10.576210958711263|3009.6381424673395|259.85663262468216|3.1098066601885823|\n",
            "|    min|                19|             -3313|                 4|                 1|\n",
            "|    max|                87|             71188|              3025|                50|\n",
            "+-------+------------------+------------------+------------------+------------------+\n",
            "\n",
            "+---+-----+\n",
            "|  y|count|\n",
            "+---+-----+\n",
            "| no| 4000|\n",
            "|yes|  521|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Data Preprocessing\n",
        "Handle Missing Values"
      ],
      "metadata": {
        "id": "j2LVt0IgSuhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing/null values per column\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()\n",
        "\n",
        "# Drop missing values for simplicity\n",
        "df = df.dropna()\n",
        "print(f\"After dropping nulls: {df.count()} rows\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-IO4PtpSsch",
        "outputId": "2babc72b-9af8-45a3-ba68-aeacd140f4f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "|age|job|marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
            "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "|  0|  0|      0|        0|      0|      0|      0|   0|      0|  0|    0|       0|       0|    0|       0|       0|  0|\n",
            "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "\n",
            "After dropping nulls: 4521 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle Outliers (Capping Extreme Balances)"
      ],
      "metadata": {
        "id": "q3Z62O3rS03_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cap balance to reduce outlier impact\n",
        "quantiles = df.approxQuantile(\"balance\", [0.01, 0.99], 0.0)\n",
        "lower, upper = quantiles\n",
        "df = df.withColumn(\"balance\", when(col(\"balance\") < lower, lower)\n",
        "                   .when(col(\"balance\") > upper, upper)\n",
        "                   .otherwise(col(\"balance\")))\n"
      ],
      "metadata": {
        "id": "u5Z4EbQ5SsaE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Encode Categorical Columns"
      ],
      "metadata": {
        "id": "adbFUxwHS6xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify categorical & numerical columns\n",
        "categorical_cols = [c for c, dtype in df.dtypes if dtype == 'string' and c not in ['y']]\n",
        "numeric_cols = [c for c, dtype in df.dtypes if dtype != 'string']\n",
        "\n",
        "# Index and OneHotEncode categorical features\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_cols]\n",
        "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\") for col in categorical_cols]\n",
        "\n",
        "# Encode target variable y\n",
        "label_indexer = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n"
      ],
      "metadata": {
        "id": "mFcHqZg6SsXi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Feature Engineering â€” Vector Assembler"
      ],
      "metadata": {
        "id": "lChGGon_S-pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all numeric and encoded categorical features\n",
        "feature_cols = [c+\"_vec\" for c in categorical_cols] + numeric_cols\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Optional scaling\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n"
      ],
      "metadata": {
        "id": "E7nN635vSsVG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Model Selection & Training\n",
        "\n",
        "Letâ€™s start with Logistic Regression, then compare with Decision Tree.\n",
        "\n",
        "Split the data"
      ],
      "metadata": {
        "id": "mBFuJj4ATB0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "print(f\"Training Data: {train_df.count()}, Test Data: {test_df.count()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca5VEcjHTF8p",
        "outputId": "a8d1e4c2-4817-44dc-e443-8fe7e9215f5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data: 3662, Test Data: 859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression Pipeline"
      ],
      "metadata": {
        "id": "h2s_p_6aTHFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"label\", maxIter=20)\n",
        "\n",
        "pipeline_lr = Pipeline(stages=indexers + encoders + [label_indexer, assembler, scaler, lr])\n",
        "model_lr = pipeline_lr.fit(train_df)\n",
        "\n",
        "predictions_lr = model_lr.transform(test_df)\n",
        "predictions_lr.select(\"y\", \"prediction\", \"probability\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT6i5eCaSsSv",
        "outputId": "a399652c-c53b-4950-f10e-e5979428e6a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------------+\n",
            "|  y|prediction|         probability|\n",
            "+---+----------+--------------------+\n",
            "| no|       0.0|[0.88758385924534...|\n",
            "| no|       0.0|[0.65320281565978...|\n",
            "| no|       0.0|[0.99132899667943...|\n",
            "| no|       0.0|[0.98772774187202...|\n",
            "| no|       0.0|[0.72548822557949...|\n",
            "+---+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Model Evaluation"
      ],
      "metadata": {
        "id": "FyRnLGCUTKl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "roc_auc = evaluator.evaluate(predictions_lr)\n",
        "print(f\"âœ… Logistic Regression AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = predictions_lr.filter(col(\"label\") == col(\"prediction\")).count() / float(test_df.count())\n",
        "print(f\"âœ… Logistic Regression Accuracy: {accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GVIcFqISry-",
        "outputId": "bd8882ce-1e4c-47e9-e219-cf1c2f6d63f7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Logistic Regression AUC: 0.8775\n",
            "âœ… Logistic Regression Accuracy: 89.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Decision Tree Model (Alternative)"
      ],
      "metadata": {
        "id": "tl8-Lr-OTSBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(featuresCol=\"scaled_features\", labelCol=\"label\")\n",
        "\n",
        "pipeline_dt = Pipeline(stages=indexers + encoders + [label_indexer, assembler, scaler, dt])\n",
        "model_dt = pipeline_dt.fit(train_df)\n",
        "predictions_dt = model_dt.transform(test_df)\n",
        "\n",
        "roc_auc_dt = evaluator.evaluate(predictions_dt)\n",
        "accuracy_dt = predictions_dt.filter(col(\"label\") == col(\"prediction\")).count() / float(test_df.count())\n",
        "\n",
        "print(f\"ðŸŒ³ Decision Tree AUC: {roc_auc_dt:.4f}\")\n",
        "print(f\"ðŸŒ³ Decision Tree Accuracy: {accuracy_dt*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ecEowouTRO1",
        "outputId": "1c5bd984-63bb-4172-f190-cb61242a9f41"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŒ³ Decision Tree AUC: 0.4696\n",
            "ðŸŒ³ Decision Tree Accuracy: 89.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Hyperparameter Tuning (Cross Validation)\n",
        "\n",
        "Weâ€™ll tune Logistic Regression."
      ],
      "metadata": {
        "id": "jmSQKGsbTRB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline_lr,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=3)\n",
        "\n",
        "cv_model = crossval.fit(train_df)\n",
        "cv_predictions = cv_model.transform(test_df)\n",
        "cv_auc = evaluator.evaluate(cv_predictions)\n",
        "print(f\"ðŸ”§ Best Model AUC after Tuning: {cv_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BSsENY-TQ1q",
        "outputId": "424c7276-877c-4baf-eb06-01eacc5b1917"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ Best Model AUC after Tuning: 0.9005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 11: Feature Importance / Coefficients\n",
        "\n",
        "For Logistic Regression:"
      ],
      "metadata": {
        "id": "QaTNmuxaTQom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_lr_model = cv_model.bestModel.stages[-1]\n",
        "coefficients = best_lr_model.coefficients\n",
        "intercept = best_lr_model.intercept\n",
        "print(f\"Intercept: {intercept}\")\n",
        "print(\"Top 10 Feature Coefficients:\")\n",
        "print(coefficients.toArray()[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDvMQKr3TQQI",
        "outputId": "7a23a93f-aa7d-47b2-8e89-b5569a3a93b2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: -3.278796423561272\n",
            "Top 10 Feature Coefficients:\n",
            "[ 0.         -0.06972094  0.          0.          0.          0.0958017\n",
            "  0.          0.          0.          0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Decision Tree:"
      ],
      "metadata": {
        "id": "xCnnPWZRTQDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_dt_model = model_dt.stages[-1]\n",
        "print(\"Top 10 Feature Importances:\")\n",
        "print(best_dt_model.featureImportances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxGbLoSQTP3t",
        "outputId": "ea059a9f-af37-4fec-9992-c266fc5ae939"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Feature Importances:\n",
            "(51,[2,5,14,16,18,25,26,28,36,38,43,44,45,46,47],[0.005522254535258133,0.014740868096134185,0.018527550137436857,0.026584657202310596,0.011860242357688238,0.007100041545331889,0.020668326012661706,0.020458511071773045,0.06411035996986823,0.007747119089252778,0.234694737304731,0.00631114804029501,0.02498162765950108,0.01327123114127889,0.5234213258364785])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17a4d7cd",
        "outputId": "6a07d1d2-5849-46bd-b6c2-b8ffd6b0b89e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_names = []\n",
        "\n",
        "# Reconstruct feature names for categorical columns (from StringIndexer and OneHotEncoder)\n",
        "# The first set of stages in the pipeline are the StringIndexers, followed by OneHotEncoders.\n",
        "# We need the labels from the StringIndexers.\n",
        "\n",
        "# Get the list of all stages from the best model of the cross-validator\n",
        "all_pipeline_stages = cv_model.bestModel.stages\n",
        "\n",
        "# Categorical columns were processed first by StringIndexers, then OneHotEncoders.\n",
        "# The first len(categorical_cols) stages are StringIndexers.\n",
        "for i, col_name in enumerate(categorical_cols):\n",
        "    # Get the fitted StringIndexerModel for this column\n",
        "    string_indexer_model = all_pipeline_stages[i]\n",
        "    labels = string_indexer_model.labels # These are the distinct categories\n",
        "\n",
        "    # OneHotEncoder typically drops the last category to avoid multicollinearity,\n",
        "    # but it appears in this pipeline setup, it's encoding all categories.\n",
        "    # So, the number of output features for an OHE column is len(labels).\n",
        "    # We create feature names for these (e.g., 'job_blue-collar').\n",
        "    for j in range(len(labels)): # Changed from len(labels) - 1 to len(labels)\n",
        "        feature_names.append(f\"{col_name}_{labels[j]}\")\n",
        "\n",
        "# Append numerical feature names\n",
        "for col_name in numeric_cols:\n",
        "    feature_names.append(col_name)\n",
        "\n",
        "# Get the coefficients from the final Logistic Regression model\n",
        "coefficients_array = cv_model.bestModel.stages[-1].coefficients.toArray()\n",
        "\n",
        "# Ensure the number of generated feature names matches the number of coefficients\n",
        "if len(feature_names) != len(coefficients_array):\n",
        "    print(f\"Warning: Mismatch between number of feature names ({len(feature_names)}) and coefficients ({len(coefficients_array)}).\")\n",
        "else:\n",
        "    # Create a list of (feature_name, coefficient) tuples\n",
        "    feature_coefficient_pairs = list(zip(feature_names, coefficients_array))\n",
        "\n",
        "    # Sort by absolute coefficient value to identify the most influential features\n",
        "    feature_coefficient_pairs.sort(key=lambda x: np.abs(x[1]), reverse=True)\n",
        "\n",
        "    print(\"Top 10 most influential features (Logistic Regression):\")\n",
        "    for name, coef in feature_coefficient_pairs[:10]:\n",
        "        print(f\"{name}: {coef:.4f}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most influential features (Logistic Regression):\n",
            "duration: 0.8848\n",
            "poutcome_success: 0.3356\n",
            "contact_unknown: -0.3272\n",
            "month_oct: 0.1890\n",
            "month_mar: 0.1278\n",
            "education_tertiary: 0.0995\n",
            "job_retired: 0.0958\n",
            "job_student: 0.0779\n",
            "job_blue-collar: -0.0697\n",
            "poutcome_unknown: -0.0647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "033b3de0",
        "outputId": "f3e38c4e-b40b-4e27-f56a-ff86b3be64e7"
      },
      "source": [
        "# Get feature importances from the best Decision Tree model\n",
        "dt_feature_importances = model_dt.stages[-1].featureImportances.toArray()\n",
        "\n",
        "# Ensure the number of generated feature names matches the number of importances\n",
        "if len(feature_names) != len(dt_feature_importances):\n",
        "    print(f\"Warning: Mismatch between number of feature names ({len(feature_names)}) and Decision Tree importances ({len(dt_feature_importances)}).\")\n",
        "else:\n",
        "    # Create a list of (feature_name, importance) tuples\n",
        "    feature_importance_pairs = list(zip(feature_names, dt_feature_importances))\n",
        "\n",
        "    # Sort by importance value to identify the most influential features\n",
        "    feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(\"\\nTop 10 most influential features (Decision Tree):\")\n",
        "    for name, importance in feature_importance_pairs[:10]:\n",
        "        print(f\"{name}: {importance:.4f}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 most influential features (Decision Tree):\n",
            "duration: 0.5234\n",
            "poutcome_success: 0.2347\n",
            "month_oct: 0.0641\n",
            "education_tertiary: 0.0266\n",
            "balance: 0.0250\n",
            "contact_unknown: 0.0207\n",
            "month_may: 0.0205\n",
            "marital_divorced: 0.0185\n",
            "job_retired: 0.0147\n",
            "day: 0.0133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ace49fe7"
      },
      "source": [
        "### Model Performance Comparison\n",
        "\n",
        "We trained and evaluated two models:\n",
        "\n",
        "1.  **Logistic Regression**\n",
        "    *   **AUC:** 0.8775\n",
        "    *   **Accuracy:** 89.64%\n",
        "    *   **Tuned Logistic Regression (with Cross-Validation)**\n",
        "        *   **AUC:** 0.9005\n",
        "\n",
        "2.  **Decision Tree**\n",
        "    *   **AUC:** 0.4696\n",
        "    *   **Accuracy:** 89.29%\n",
        "\n",
        "From these results, the **Tuned Logistic Regression model** performed significantly better, achieving an AUC of **0.9005**, compared to the Decision Tree's AUC of 0.4696. While the accuracies are similar, AUC provides a better measure for imbalanced datasets, indicating that Logistic Regression is more effective at distinguishing between the positive and negative classes."
      ]
    }
  ]
}